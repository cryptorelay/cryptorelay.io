交易系统架构
============

确定性系统
------

对于交易系统架构设计来说，交易系统最重要的特点是确定性。我们要对一个规则确定，不能出错的逻辑，用分布式的方式实现它，以达到高性能和高可用的目的。

-   确定性意味着从撮合到清算到行情计算，整个系统是从输入到输出的纯函数，任何时候给定相同的输入，总能得到相同的输出。

    整个系统都可以看作是输入数据的衍生数据。

-   输入是所有用户的下单请求序列(下单，撤单，充值，提现等等)，输出是成交记录、资产变更、行情变化等。

-   确定性意味着：

    -   请求的顺序必须是确定的。
    -   计算过程中不能依赖其他副作用（比如本地时钟，随机数等等），尤其是生成一些唯一ID的时候要特别注意，生成逻辑必须只跟输入有关。

-   确定性是后面整个架构设计的前提。

纯函数
------

确定性用函数式编程的语言来说就是纯函数。

撮合函数的输入是下单和撤单请求流，它的输出是什么呢，我认为可以定义为一次下单的Taker和成交的Maker列表。
后续的所有逻辑，包括订单入库和状态变更，成交记录-\>K线行情，用户资产清算，只需要依赖这一个输入就都能够满足。

``` {.haskell}
type Taker = Order
type Maker = Order
data Output = Trade Taker [Maker]
            | Cancel Order
match :: Order -> State OrderBook Output
clear_trade :: Output -> [TradeRecord]
clear_asset :: Output -> [(Debitor, Creditor, Coin, Amount)]
clear_order :: Output -> (OrderInserts, OrderUpdates, OrderMoves)
```

源头重放
--------

在做到确定性的前提下，任何时候把历史输入重新跑一遍即可重现当前状态(以及历史中任何时刻的状态)。意味着计算节点就算不做任何持久化，故障也不会引起任何数据丢失。

高可用
------

计算节点的逻辑是确定性的，相同的输入总是产生相同的输出，所以可以很容易起多个同时执行，下游节点选择其中任何一个进行订阅，都能得到相同的结果。撮合引擎因为在最前端，也可以采用主从复制的形式起多个实例，master挂掉的时候通过raft算法进行failover。

Push vs Pull
------------

计算节点和结果流是一体，还是分离？

一般开发团队为了省事重用现有消息组件，会选择分离的方案，也就是计算节点需要主动把结果Push给外部消息组件。

但是这样就要考虑Push出去的消息是否丢失的问题（比如使用kafka就意味着配置ack=all以及retres=MAX\_VALUE等等），最重要的是Push模式下，节点记录的最后一个成功消费的输入ID，必须保证是Push成功过的，只有做到这一点，Push模式才是可靠的。

Pull模式就是计算节点需要自己实现一个输出流，供下游节点主动订阅，从可靠性以及低延迟的角度，显然还是Pull模式要更靠谱一些，Pull模式实现上难度更大，但这对我们来说不是问题。

批量 vs 单条
------------

高吞吐和低延迟是矛盾的，我们只能在其中进行权衡。通常的设计是把实时的处理变成定时的处理，但这样会给用户带来很卡顿的感觉。

我们目前的实现机制能够做到自动适应不同负载，进行不同规模的批量执行。也就是低负载的时候，系统也以最低的延迟运行，用户能够得到实时用户体验。当负载逐渐增长时，吞吐量增大，延迟也会相应有所升高。我们测试在单币对每秒5w以下时，延迟都在100ms以内，对web用户来说，已经是接近实时的体验了。

快照
----

为了加快故障恢复的时间，计算节点还是要对自身状态进行快照持久化。状态中需要包含对输入处理的当前进度，故障恢复时只需要从上次的进度继续执行即可。

幂等
----

当上游节点发生故障恢复或者failover的时候，下游节点会遇到消息重复消费的情况，也就是所有计算节点需要做到幂等。

当我们的输入是确定且有序时，可以给他们附带严格有序递增的ID，下游节点只需要记录处理过的最后一个ID，即可实现幂等。

流水线
------

同样因为对确定性的要求，撮合引擎节点很难通过水平扩展提升吞吐量，只能尽可能以多线程流水线式的处理，充分利用多核计算性能。

过载保护
--------

请求量超过系统处理能力后，需要主动拒绝请求，否则排队队列太长，延迟飙升。这个需要跟踪当前消费进度以及队列长度，由前端(是后端的前端，不是前端那个前端)进行判断。

请求优先级
----------

在排队的时候，不同交易请求可以有优先级的区别，比如撤单请求的优先级应该高过下单，高价值的单优先等等。优先级高的请求能插队。插队位置只能插在未消费过的位置。

实现机制
--------

-   基于日志的队列有kafka和redis
    stream，但是一些高级功能(比如请求优先级)并没有，需要自己定制实现。

-   因为一些计算节点必须顺序执行(撮合)，必须充分发挥单机甚至单核的计算能力，这方面C/C++依然是最佳选择。

    针对这个场景Redis
    Module是一个不错的选择，能够复用redis本身的协议以及持久化和replication机制，方便提供查询的接口，自动就有了读写分离的能力。节点之间也可以通过redis
    stream可靠地传输数据。

    性能方面要再上一层，Redis本身还是太过单线程，如果专门写一个share
    nothing的多线程流水线结构的服务，肯定还能进一步提升吞吐量，这方面c++的
    [seastar](http://seastar.io) 框架是个不错的选择。

-   关系数据库

    关系数据库虽然不快，但是实用。关系数据库的特点是单核性能很一般，但是能够垂直扩展。可以用来存储历史流水记录供查询用，包括订单历史、成交记录、资金流水，这些可以根据时间自动分表，甚至使用timescaledb这种专门针对时序数据的插件，可以提供强大的插入吞吐量。用户资产表也可以考虑放。k线计算，利用pipelinedb的流聚合能力，也可以很容易完成。

资产模块
--------

以上整套系统都是按照币对分区的，但是用户资产余额的存储只能按照币或用户ID分区，中间的Worker需要处理两头分区的交错。

Worker的基本工作思路还是一样，订阅上游流的输出，过滤出感兴趣的那个币相关的资产变更信息。
在一个事务里面，批量修改用户资产数据，同时记录上游消费记录。

Worker的数量可以与下游分区对应，这样意味着一个Worker可能需要同时订阅多个上游流，这样的好处是下游操作不存在并发。
另一方面也可以与上游分区对应，一个币对对应两个Worker，这样多个Worker会并发访问下游存储，也问题不大，注意对user\_id排个序，防止死锁。
关系数据库能够很好的处理并发访问，所以倾向于第二个方案，尤其是考虑到不同币对的user\_id重合度不高。

充值/提现当作一个独立的输入流来处理，处理逻辑类似。

这样不同的输入流之间是并发处理的，但是并不存在问题，原因有两个：

1. 所有数据库操作都做成自增自减的形式，在read commited事务隔离模式下是原子的。
2. 提现和下单都有个提前冻结的操作，后续操作都只会造成可用余额的增加或者不变，所以不会出现可用余额为负数的情况。

传统实现方案里，还容易出现一种特殊账户(比如手续费账户)资产记录成为并发热点，导致性能下降，也可以使用独立的worker进行计算和更新。其实在自动批量处理的设计模式下，根据我们的测试，直接处理也并没有什么问题。

对账
----

对账的目的是对程序计算结果的验算，一方面是对撮合本身的输出进行校验，另一方面是对资产计算结果的校验。
Coming soon...

自成交避免
----------

为了避免自成交行为对行情的操控，应该对相同用户的成交情况做特殊处理。目前在撮合部分正常撮合，清算的时候识别自成交的订单，避免计算到行情里去。

数值计算
--------

OrderBook数据结构选择
--------
